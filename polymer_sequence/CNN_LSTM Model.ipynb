{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten, Dropout, Permute, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import History\n",
    "import keras_tuner\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from sklearn.model_selection import KFold\n",
    "from kerastuner import HyperModel, RandomSearch, Hyperband\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load('combined_labels.npy')\n",
    "X = np.load('combined_features.npy')\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "y_scaled = scaler.fit_transform(y)\n",
    "rows_to_remove = [87, 88, 89, 90]\n",
    "X_input = np.delete(X, rows_to_remove, axis=0)\n",
    "X_predictions = X[87:91]\n",
    "y_input = np.delete(y_scaled, rows_to_remove, axis=0)\n",
    "y_predictions = y_scaled[87:91]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    SS_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - SS_res / (SS_tot + tf.keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, initial_learning_rate, total_steps):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=initial_learning_rate,  \n",
    "        decay_steps=total_steps,  \n",
    "        alpha=0.0 \n",
    "    )\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Permute((2, 1)),\n",
    "        Conv1D(36, kernel_size=3,padding='same', kernel_regularizer = regularizers.l1(3e-5)),\n",
    "        LeakyReLU(negative_slope=0.2),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(56, kernel_size=3,padding='same', kernel_regularizer = regularizers.l1(1e-4)),\n",
    "        LeakyReLU(negative_slope=0.2),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(36, kernel_size=3, padding='same', kernel_regularizer=regularizers.l1(6e-3)),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=regularizers.l2(2e-3)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),\n",
    "        Dense(160, activation='relu', kernel_regularizer=regularizers.l1(4e-3)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae', r_squared])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(input_shape, initial_learning_rate, total_steps):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=initial_learning_rate,  \n",
    "        decay_steps=total_steps,  \n",
    "        alpha=0.0 \n",
    "    )\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Permute((2, 1)),\n",
    "        Conv1D(44, kernel_size=3, padding='same', kernel_regularizer = regularizers.l2(2e-5)),\n",
    "        LeakyReLU(negative_slope=0.2),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Permute((2, 1)),\n",
    "        LSTM(72, return_sequences=True, kernel_regularizer=regularizers.l1(6e-3)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, return_sequences=True, kernel_regularizer=regularizers.l1(7e-5)),  \n",
    "        Dropout(0.2),\n",
    "        LSTM(56, return_sequences=False, kernel_regularizer=regularizers.l1(8e-4)),  \n",
    "        Dropout(0.4),\n",
    "        Flatten(),\n",
    "        Dense(192, activation='relu', kernel_regularizer=regularizers.l1(1e-3)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae', r_squared])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, initial_learning_rate, total_steps):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=initial_learning_rate,  \n",
    "        decay_steps=total_steps, \n",
    "        alpha=0.0 \n",
    "    )\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(16, return_sequences=True, kernel_regularizer=regularizers.l1(1e-4)),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64, return_sequences=True, kernel_regularizer=regularizers.l2(4e-4)),  \n",
    "        Dropout(0.3),\n",
    "        LSTM(88, return_sequences=False, kernel_regularizer=regularizers.l1(2e-4)),  \n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(96, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae', r_squared])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "total_steps = (len(X_train) // 64) * 100\n",
    "model1 = create_cnn_lstm_model(input_shape, initial_learning_rate=0.005, total_steps=total_steps)\n",
    "history1 = History()\n",
    "model1.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), callbacks=[history])\n",
    "\n",
    "model2 = create_cnn_model(input_shape, initial_learning_rate=0.002, total_steps=total_steps)\n",
    "history2 = History()\n",
    "model2.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), callbacks=[history])\n",
    "\n",
    "model3 = create_lstm_model(input_shape, initial_learning_rate=0.001, total_steps=total_steps)\n",
    "history3 = History()\n",
    "model3.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), callbacks=[history])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
